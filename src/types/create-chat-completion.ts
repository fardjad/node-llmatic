export type CreateChatCompletionRequest = {
  /**
   * Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing
   * frequency in the text so far, decreasing the model's likelihood to repeat the same line
   * verbatim.
   *
   * [See more information about frequency and presence
   * penalties.](/docs/api-reference/parameter-details)
   */
  frequency_penalty?: number;
  /**
   * Modify the likelihood of specified tokens appearing in the completion.
   *
   * Accepts a json object that maps tokens (specified by their token ID in the tokenizer) to
   * an associated bias value from -100 to 100. Mathematically, the bias is added to the
   * logits generated by the model prior to sampling. The exact effect will vary per model,
   * but values between -1 and 1 should decrease or increase likelihood of selection; values
   * like -100 or 100 should result in a ban or exclusive selection of the relevant token.
   */
  logit_bias?: { [key: string]: any };
  /**
   * The maximum number of tokens allowed for the generated answer. By default, the number of
   * tokens the model can return will be (4096 - prompt tokens).
   */
  max_tokens?: number;
  /**
   * The messages to generate chat completions for, in the [chat
   * format](/docs/guides/chat/introduction).
   */
  messages: MessageElement[];
  /**
   * ID of the model to use. Currently, only `gpt-3.5-turbo` and `gpt-3.5-turbo-0301` are
   * supported.
   */
  model: string;
  /**
   * How many chat completion choices to generate for each input message.
   */
  n?: number;
  /**
   * Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they
   * appear in the text so far, increasing the model's likelihood to talk about new topics.
   *
   * [See more information about frequency and presence
   * penalties.](/docs/api-reference/parameter-details)
   */
  presence_penalty?: number;
  /**
   * Up to 4 sequences where the API will stop generating further tokens.
   */
  stop?: string[] | string;
  /**
   * If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as
   * data-only [server-sent
   * events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format)
   * as they become available, with the stream terminated by a `data: [DONE]` message.
   */
  stream?: boolean;
  /**
   * What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the
   * output more random, while lower values like 0.2 will make it more focused and
   * deterministic.
   *
   * We generally recommend altering this or `top_p` but not both.
   */
  temperature?: number;
  /**
   * An alternative to sampling with temperature, called nucleus sampling, where the model
   * considers the results of the tokens with top_p probability mass. So 0.1 means only the
   * tokens comprising the top 10% probability mass are considered.
   *
   * We generally recommend altering this or `temperature` but not both.
   */
  top_p?: number;
  /**
   * A unique identifier representing your end-user, which can help OpenAI to monitor and
   * detect abuse. [Learn more](/docs/guides/safety-best-practices/end-user-ids).
   */
  user?: string;
  [property: string]: any;
};

export type MessageElement = {
  /**
   * The contents of the message
   */
  content: string;
  /**
   * The name of the user in a multi-user chat
   */
  name?: string;
  /**
   * The role of the author of this message.
   */
  role: Role;
  [property: string]: any;
};

/**
 * The role of the author of this message.
 */
export enum Role {
  Assistant = "assistant",
  System = "system",
  User = "user",
}

export type CreateChatCompletionOkResponse = {
  choices: Choice[];
  created: number;
  id: string;
  model: string;
  object: string;
  usage?: Usage;
  [property: string]: any;
};

export type Choice = {
  finish_reason?: string;
  index?: number;
  message?: ChoiceMessage;
  [property: string]: any;
};

export type ChoiceMessage = {
  /**
   * The contents of the message
   */
  content: string;
  /**
   * The role of the author of this message.
   */
  role: Role;
  [property: string]: any;
};

export type Usage = {
  completion_tokens: number;
  prompt_tokens: number;
  total_tokens: number;
  [property: string]: any;
};
